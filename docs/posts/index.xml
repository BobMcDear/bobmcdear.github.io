<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on AI Blog</title>
    <link>https://bobmcdear.github.io/posts/</link>
    <description>Recent content in Posts on AI Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>borna.ahz@gmail.com (Borna Ahmadzadeh)</managingEditor>
    <webMaster>borna.ahz@gmail.com (Borna Ahmadzadeh)</webMaster>
    <lastBuildDate>Mon, 03 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://bobmcdear.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Attention in Vision</title>
      <link>https://bobmcdear.github.io/posts/attention-in-vision/</link>
      <pubDate>Mon, 03 Apr 2023 00:00:00 +0000</pubDate>
      <author>borna.ahz@gmail.com (Borna Ahmadzadeh)</author>
      <guid>https://bobmcdear.github.io/posts/attention-in-vision/</guid>
      <description>&lt;p&gt;Attention is an influential mechanism in deep learning that has achieved state-of-the-art results in many domains such as natural language processing, visual understanding, speech recognition, multi-modal learning, and more. In this article, popular attention layers in computer vision specifically will be studied and implemented in PyTorch.
Each module is accompanied by detailed descriptions, visualizations, and code, plus optional mathematical formulations for those preferring more formal expositions.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
